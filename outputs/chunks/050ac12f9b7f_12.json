{
  "content": "ame\" ]; then\n        echo \"* $session_name (current)\"\n      else\n        echo \"  $session_name\"\n      fi\n    fi\n  done\n}\n\n# Switch to a session\nswitch_session() {\n  local session_name=\"$1\"\n  local session_dir=\"${SESSION_DIR}/${session_name}\"\n  \n  if [ ! -d \"$session_dir\" ]; then\n    echo \"Session does not exist: $session_name\"\n    return 1\n  fi\n  \n  echo \"$session_name\" > \"$CURRENT_SESSION_FILE\"\n  echo \"Switched to session: $session_name\"\n}\n\n# Get current session\nget_current_session() {\n  if [ -f \"$CURRENT_SESSION_FILE\" ]; then\n    cat \"$CURRENT_SESSION_FILE\"\n  else\n    echo \"default\"\n  fi\n}\n\n\n### 6. Metadata Enhancement\n\nAdd metadata tracking to better manage your chunks:\n\npython\n# In file_chunker.py, modify to include metadata\nchunk_data = {\n    \"content\": chunk_text,\n    \"metadata\": {\n        \"source_file\": os.path.basename(input_file),\n        \"chunk_id\": f\"{file_prefix}_{chunk_index}\",\n        \"start_char\": start_pos,\n        \"end_char\": end_pos,\n        \"created_date\": datetime.now().isoformat(),\n        \"chunk_size\": len(chunk_text),\n        \"token_estimate\": len(chunk_text.split()) * 1.3  # Rough estimate\n    }\n}\n\n\n### 7. Local Embedding Improvement\n\nIf you're using sentence-transformers for semantic search, use a more efficient model:\n\npython\n# In rag_assistant.py\nfrom sentence_transformers import SentenceTransformer\n\n# Use a smaller, more efficient model\nmodel = SentenceTransformer('all-MiniLM-L6-v2')  # Only 80MB vs 420MB for larger models\n\n\n### 8. Add Caching for Better Performance\n\nImplement a simple caching mechanism for embeddings:\n\npython\n# In rag_assistant.py\nimport os\nimport pickle\nimport hashlib\n\ndef get_cached_embeddings(chunks_dir, cache_dir):\n    \"\"\"Get embeddings from cache or compute if not available\"\"\"\n    # Create cache dir if it doesn't exist\n    os.makedirs(cache_dir, exist_ok=True)\n    \n    # Hash the chunks directory to identify this set of chunks\n    dir_hash = hashlib.md5(chunks_dir.encode()).hexdigest()\n    cache_file = os.path.join(",
  "metadata": {
    "source": "/mnt/chromeos/removable/USB Drive/review_assistant/codebase/chat_sessions/Chat_with_CFO_20250521_140836.txt",
    "chunk_id": "050ac12f9b7f_12",
    "start_char": 21600,
    "end_char": 23600,
    "created_at": "2025-05-21T08:41:31+00:00"
  }
}