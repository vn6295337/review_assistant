{
  "content": "of just showing a preview of the chunk).\n\nYou said:\nyes\nChatGPT said:\nGreat — here’s how we’ll extend your script to summarize each chunk using a local or online model, instead of just previewing the first 20 lines.\n\n✅ Plan to Upgrade file_summarizer.py with Model-Based Summarization\n🧠 Step 1: Add a summarization model\nFor now, we’ll support two options:\n\nOffline (local model) — if available, like transformers + t5-small\n\nOnline (OpenAI or other) — if you want, but let’s keep it offline-first\n\n🛠️ Modified Code: Uses Local HuggingFace Model (t5-small)\nRequirements:\npip install transformers torch\n✨ New file_summarizer.py (Model-Based)\n#!/usr/bin/env python3\n\"\"\"\nFile Summarizer Script (Model-based)\n\nUses a transformer model to summarize content chunks from JSON files.\n\"\"\"\n\nimport os\nimport json\nimport argparse\n\nfrom transformers import pipeline\n\n# Load the summarization model only once\nsummarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\")\n\n\ndef summarize_text(text, max_tokens=512):\n    \"\"\"\n    Use the Hugging Face summarizer to generate a summary.\n    \"\"\"\n    # Truncate long input\n    input_text = text.strip().replace(\"\\n\", \" \")\n    if len(input_text) > max_tokens * 4:\n        input_text = input_text[:max_tokens * 4]\n\n    summary = summarizer(input_text, max_length=100, min_length=20, do_sample=False)\n    return summary[0]['summary_text']\n\n\ndef summarize_chunk(input_file, output_dir):\n    try:\n        os.makedirs(output_dir, exist_ok=True)\n        base_name = os.path.basename(input_file)\n        output_file = os.path.join(output_dir, f\"summary_{base_name}\")\n\n        with open(input_file, 'r', encoding='utf-8') as f:\n            chunk_data = json.load(f)\n\n        content = chunk_data.get('content')\n        if content is None:\n            print(f\"⚠ Error: 'content' field missing in {input_file}\")\n            return False\n\n        chunk_id = chunk_data.get('chunk_id', 'unknown')\n        source_file = chunk_data.get('file', 'unknown')\n\n        # Su",
  "metadata": {
    "source": "/mnt/chromeos/removable/USB Drive/review_assistant/codebase/chat_sessions/Chat_with_CFO_20250521_140836.txt",
    "chunk_id": "050ac12f9b7f_201",
    "start_char": 361800,
    "end_char": 363800,
    "created_at": "2025-05-21T08:41:33+00:00"
  }
}