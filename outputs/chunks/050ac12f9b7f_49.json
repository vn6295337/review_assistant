{
  "content": " break\n        res = search(q, chunks, model, k)\n        for i,(chunk,score) in enumerate(res,1):\n            snippet = chunk[\"text\"][:500] + (\"...\" if len(chunk[\"text\"])>500 else \"\")\n            print(f\"\\n#{i} ‚Ä¢ {chunk['file']} ‚Ä¢ score‚ÄØ{score:.2f}\\n{snippet}\")\n\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--chunks-dir\", default=DEFAULT_CHUNKS, help=\"Directory with chunks\")\n    ap.add_argument(\"--top-k\", type=int, default=5, help=\"Number of results\")\n    args = ap.parse_args()\n\n    dir_path = Path(args.chunks_dir)\n    if not dir_path.is_dir():\n        print(f\"‚ùå {dir_path} not found\"); return\n    chunks = load_chunks(dir_path)\n    if not chunks:\n        print(\"‚ùå No chunks found.\"); return\n\n    print(\"üì° Loading SBERT model‚Ä¶\")\n    from sentence_transformers import SentenceTransformer\n    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n\n    chunks = build_embeddings(chunks, model)\n    interactive(chunks, model, args.top_k)\n\nif __name__ == \"__main__\":\n    main()\nHow these changes help\nZero duplicate configs ‚Äì both scripts honour the same config.sh.\n\nClear naming ‚Äì ‚Äúsimple‚Äù vs ‚Äúsbert‚Äù.\n\nConsistent CLI ‚Äì --chunks-dir everywhere, defaulting to the same place.\n\nGraceful failures ‚Äì scripts exit with readable messages if chunks are missing.\n\nFeel free to tweak model names, defaults, or stop‚Äëword lists as you like. Let me know if you‚Äôd like patches for file_summarizer.py (to add --skip-existing) or any other module!\n\nYou said:\nfile_chunker.py in the same folder review_assistant/python. refactor it\n\n#!/usr/bin/env python3\n\"\"\"\nFile Chunker Script\n\nThis script breaks down large files into manageable chunks for processing.\nIt's designed to handle files that exceed LLM context windows.\n\"\"\"\n\nimport os\nimport json\nimport sys\nimport argparse\nimport hashlib\nfrom pathlib import Path\n\ndef chunk_file(input_file, output_dir, chunk_size=2000, overlap=200, verbose=False):\n    \"\"\"\n    Break down a file into overlapping chunks.\n    \n    Args:\n        input_file (str): Pat",
  "metadata": {
    "source": "/mnt/chromeos/removable/USB Drive/review_assistant/codebase/chat_sessions/Chat_with_CFO_20250521_140836.txt",
    "chunk_id": "050ac12f9b7f_49",
    "start_char": 88200,
    "end_char": 90200,
    "created_at": "2025-05-21T08:41:32+00:00"
  }
}