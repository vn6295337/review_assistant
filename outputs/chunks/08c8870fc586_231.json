{
  "content": "hon:\n1. simple_rag_assistant.py:\n#!/usr/bin/env python3\n\nimport os\nimport glob\nimport json\nimport argparse\nimport sys\nimport math\nimport re\nfrom pathlib import Path\nfrom datetime import datetime\nimport readline  # For better command line input handling\n\nclass SimpleVectorizer:\n    \"\"\"A simple text vectorizer that doesn't require external libraries\"\"\"\n    \n    def __init__(self):\n        # Common English stop words to ignore\n        self.stop_words = {\n            'a', 'an', 'the', 'and', 'or', 'but', 'if', 'because', 'as', 'what',\n            'when', 'where', 'how', 'which', 'who', 'whom', 'this', 'that', 'these',\n            'those', 'then', 'just', 'so', 'than', 'such', 'both', 'through',\n            'about', 'for', 'is', 'of', 'while', 'during', 'to', 'from', 'in',\n            'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then',\n            'once', 'here', 'there', 'all', 'any', 'both', 'each', 'few', 'more',\n            'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own',\n            'same', 'so', 'than', 'too', 'very', 'can', 'will', 'just', 'should',\n            'now', 'with', 'by', 'be', 'been', 'being', 'am', 'are', 'was', 'were'\n        }\n        \n        # Dictionary to store term frequencies across all documents\n        self.document_count = 0\n        self.term_doc_freq = {}\n        self.vocabulary = set()\n        \n    def tokenize(self, text):\n        \"\"\"Convert text to lowercase and split into tokens\"\"\"\n        # Remove special characters and convert to lowercase\n        text = re.sub(r'[^\\w\\s]', ' ', text.lower())\n        # Split into tokens and filter out stop words\n        tokens = [word for word in text.split() if word not in self.stop_words and len(word) > 1]\n        return tokens\n    \n    def get_term_frequency(self, text):\n        \"\"\"Calculate term frequency for a text\"\"\"\n        tokens = self.tokenize(text)\n        term_freq = {}\n        \n        # Count term frequencies\n        for token in tokens:\n            if toke",
  "metadata": {
    "source": "/mnt/chromeos/removable/USB Drive/review_assistant/codebase/chat_sessions/Chat_with_CFO_20250521_125041.txt",
    "chunk_id": "08c8870fc586_231",
    "start_char": 415800,
    "end_char": 417800,
    "created_at": "2025-05-21T07:21:41+00:00"
  }
}