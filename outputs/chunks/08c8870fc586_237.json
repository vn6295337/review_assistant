{
  "content": "    print(\"Goodbye!\")\n                break\n                \n            if not question:\n                continue\n                \n            start_time = datetime.now()\n            answer = assistant.answer_question(question)\n            end_time = datetime.now()\n            \n            print(\"\\nAnswer:\")\n            print(answer)\n            print(f\"\\n(Response generated in {(end_time - start_time).total_seconds():.2f} seconds)\")\n            \n        except KeyboardInterrupt:\n            print(\"\\nGoodbye!\")\n            break\n        except Exception as e:\n            print(f\"Error: {e}\")\n\nif __name__ == \"__main__\":\n    main()\n\n2. rag_assistant .py:\nimport argparse\nimport json\nfrom pathlib import Path\nfrom sentence_transformers import SentenceTransformer, util\n\ndef load_chunks_from_directory(directory_path):\n    \"\"\"Load all JSON chunk files from the specified directory.\"\"\"\n    chunk_list = []\n    for json_file in Path(directory_path).glob(\"*.json\"):\n        with open(json_file, mode=\"r\", encoding=\"utf-8\") as file:\n            content = json.load(file)\n            text_content = content.get(\"text\") if isinstance(content, dict) else content\n            chunk_list.append({\"text\": text_content, \"file\": json_file.name})\n    return chunk_list\n\ndef compute_embeddings(chunks, embedding_model):\n    \"\"\"Generate embeddings for the provided list of chunks.\"\"\"\n    texts = [chunk[\"text\"] for chunk in chunks]\n    embeddings = embedding_model.encode(texts, convert_to_tensor=True)\n    for chunk, embedding in zip(chunks, embeddings):\n        chunk[\"embedding\"] = embedding\n    return chunks\n\ndef perform_similarity_search(query, embedded_chunks, embedding_model, top_k=5):\n    \"\"\"Search the top_k most relevant chunks given a user query.\"\"\"\n    query_vector = embedding_model.encode(query, convert_to_tensor=True)\n    similarity_scores = util.cos_sim(query_vector, [c[\"embedding\"] for c in embedded_chunks])[0]\n    top_indices = similarity_scores.argsort(descending=True)[:top_k]\n    retu",
  "metadata": {
    "source": "/mnt/chromeos/removable/USB Drive/review_assistant/codebase/chat_sessions/Chat_with_CFO_20250521_125041.txt",
    "chunk_id": "08c8870fc586_237",
    "start_char": 426600,
    "end_char": 428600,
    "created_at": "2025-05-21T07:21:41+00:00"
  }
}