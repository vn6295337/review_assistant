{
  "source_file": "/mnt/chromeos/removable/USB Drive/review_assistant/outputs/chunks/050ac12f9b7f_131.json",
  "summary": "### Chunk unknown \u2014 unknown\n\ner Chromebook Linux setup with limited local storage and no access to persistent LLM memory, file uploads, cloud compute, APIs, or vector DBs. The issues I currently face are as follows: 1. Context Window Management: Free tiers of ChatGPT and Claude cannot directly access your USB files. Every piece of content must be manually copied/pasted, which is impractical for large codebases. 2. Session Persistence: Free-tier models have no memory between sessions, forcing you to reestablish context repeatedly. 3. File Size Issues: 2+ million token files far exceed all free-tier context windows (typically 4K-32K tokens). 4. Manual Overhead: Managing file fragments and orchestrating the conversation flow manually would be extremely tedious and error-prone. 5. Chromebook Constraints: Linux on Chromebook has limited resources for running computationally intensive processes locally. My idea is to store large (e.g. >2M tokens) codebases, prompts, and notes on a USB drive mounted at /mnt/chromeos/removable/USB Drive/review_assistant/, and enable context-efficient offline review of these large files using RAG + prompt engineering strategies within the constraints of free LLMs like ChatGPT and Claude. \n\nSeems like  we have reviewed all the required files for this purpose. Can you trace the workflow, right from what users has to do from the beginning upto the final output generated, including decision points. \n\nUse simple visualization similar to the one shown below.\nreview_assistant/\n\u251c\u2500\u2500 backups/              # Backup copies of important scripts\n\u251c\u2500\u2500 chunks/               # Raw chunks of processed files\n\u251c\u2500\u2500 codebase/             # Your code files and chat sessions\n\u2502   \u2514\u2500\u2500 chat_sessions/    # Exported AI chat sessions\n\u251c\u2500\u2500 docs/                 # Documentation files\n\u251c\u2500\u2500 outputs/              # Generated outputs\n\u2502   \u251c\u2500\u2500 chunks/           # Processed chunks\n\u2502   \u251c\u2500\u2500 prompts/          # Generated prompts for AI tools\n\u2502   \u2514\u2500\u2500 summaries/        # Summaries of chunks\n\u251c\u2500\u2500 python/\n\n\u2026"
}