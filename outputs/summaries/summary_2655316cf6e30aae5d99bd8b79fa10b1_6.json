{
  "source_file": "/mnt/chromeos/removable/USB Drive/review_assistant/chunks/2655316cf6e30aae5d99bd8b79fa10b1_6.json",
  "summary": "## Chunk 6 from /tmp/claude_chat.txt (ID: 2655316cf6e30aae5d99bd8b79fa10b1_6)\n\n\n\nfull_rag_workflow.sh\n87 lines\n\nsh\n\nI'm a beginner developer building an online market research tool with the help of free tiers of ChatGPT, Claude, Gemini, etc. and having free-tier Chromebook Linux setup with limited local storage and no access to persistent LLM memory, file uploads, cloud compute, APIs, or vector DBs. The issues I currently face are as follows: 1. Context Window Management: Free tiers of ChatGPT and Claude cannot directly access your USB files. Every piece of content must be manually copied/pasted, which is impractical for large codebases. 2. Session Persistence: Free-tier models have no memory between sessions, forcing you to reestablish context repeatedly. 3. File Size Issues: 2+ million token files far exceed all free-tier context windows (typically 4K-32K tokens). 4. Manual Overhead: Managing file fragments and orchestrating the conversation flow manually would be extremely tedious and error-prone. 5. Chromebook Constraints: Linux on Chromebook has limited resources for running computationally intensive processes locally. My idea is to store large (e.g. >2M tokens) codebases, prompts, and notes on a USB drive mounted at /mnt/chromeos/removable/USB Drive/review_assistant/, and enable context-efficient offline review of these large files using RAG + prompt engineering strategies within the constraints of free LLMs like ChatGPT and Claude. You are an expert software developer. You critically reviewed my original idea and instead recommended a local, file-based RAG pipeline made up of chunking, summarization, search, and structured prompt workflows using only open-source tools. You helped create necessary folders and files, including the ones below and saved it in /mnt/chromeos/removable/USB Drive/review_assistant.\n\nYou created a One\u2011Click Workflow: full_rag_workflow.sh\nRunning this single shell script does:\n1. Export Chat Pauses for you to paste your clipboard into a temp file.\n2. Clean Slate Deletes all prior chunks, summaries, prompts, and old chat exports.\n3. Chunk file_chunker.py splits your new chat (or any file) into 500\u2011token JSON pieces.\n4. Summarize file_summarizer.py generates a short JSON summary for each chunk.\n5. Build Prompt mcp_helper.py combines the chosen summary and your template into a single, timestamped .txt prompt.\n6. Result You end up with one final prompt file (plus a \u201clatest\u201d copy), ready to paste into any free\u2011tier LLM.\nAttached is the script.\n\nThe final output I'm requiring is a detailed prompt with condensed information from the original file, \\\"/tmp/claude_chat without any loss of important information.\n\n I'm confused with the next steps.\n\nEdit\nI can see your confusion. Let me help clarify the next steps for using your RAG (Retrieval-Augmented Generation) workflow with the full_rag_workflow.sh script.\n\nBased on reviewing the script, here's a step-by-step guide on how to use it:\n\nHow to Use Your RAG Workflow\nFirst, make sure the script is executable:\nbash\nchmod +x /mnt/chromeos/removable/USB\\ Drive/review_assistant/codebase/full_rag_workflow.sh\nRun the script:\nbash\n/mnt/chromeos/removable/USB\\ Drive/review_assistant/codebase/full_rag_workflow.sh\nWhen prompted, paste your Claude chat content:\nThe script will open nano editor\nPaste the chat content you want to process\nSave with Ctrl+O, then exit with Ctrl+X\nThe script will automatically:\nClean old files\nChunk your chat into smaller pieces\nSummarize those chunks\nBuild a structured prompt based on the template\nFind your output:\nThe final prompt will be saved at: /mnt/chromeos/removable/USB Drive/review_assistant/outputs/prompts/summary_prompt_[TIMESTAMP].txt\nUse the prompt with any LLM:\nOpen the generated prompt file\nCopy its contents\nPaste into Claude, ChatGPT, or any other free-tier LLM\n"
}